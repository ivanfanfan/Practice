The quicksort algorithm has a worst-case running time of ‚.n 2 / on an input array
of n numbers. Despite this slow worst-case running time, quicksort is often the
best practical choice for sorting because it is remarkably efûcient on average: its
expected running time is ‚.n lg n/ when all numbers are distinct, and the constant
factors hidden in the ‚.n lg n/ notation are small. Unlike merge sort, it also has
the advantage of sorting in place (see page 158), and it works well even in virtual-
memory environments.
Our study of quicksort is broken into four sections. Section 7.1 describes the
algorithm and an important subroutine used by quicksort for partitioning. Because
the behavior of quicksort is complex, we’ll start with an intuitive discussion of
its performance in Section 7.2 and analyze it precisely at the end of the chapter.
Section 7.3 presents a randomized version of quicksort. When all elements are
distinct, 1 this randomized algorithm has a good expected running time and no par-
ticular input elicits its worst-case behavior. (See Problem 7-2 for the case in which
elements may be equal.) Section 7.4 analyzes the randomized algorithm, showing
that it runs in ‚.n 2 / time in the worst case and, assuming distinct elements, in
expected O.n lg n/ time.
Figure 2.4 The operation of merge sort on the array A with length 8 that initially contains the
sequence h12; 3; 7; 9; 14; 6; 11; 2i. The indices p, q, and r into each subarray appear above their
values. Numbers in italics indicate the order in which the M ERGE-SORT and MERGE procedures are
called following the initial call of MERGE-SORT.A; 1; 8/.
D.n/ C aT .n=b/ C C.n/ otherwise :
Chapter 4 shows how to solve common recurrences of this form.
Sometimes, the n=b size of the divide step isn’t an integer. For example, the
MERGE-SORT procedure divides a problem of size n into subproblems of sizes
dn=2e and bn=2c. Since the difference between dn=2e and bn=2c is at most 1,
2.3 Designing algorithms
41
which for large n is much smaller than the effect of dividing n by 2, we’ll squint a
little and just call them both size n=2. As Chapter 4 will discuss, this simpliûcation
of ignoring üoors and ceilings does not generally affect the order of growth of a
solution to a divide-and-conquer recurrence.
Another convention we’ll adopt is to omit a statement of the base cases of the
recurrence, which we’ll also discuss in more detail in Chapter 4. The reason is
that the base cases are pretty much always T .n/ D ‚.1/ if n < n 0 for some
constant n 0 > 0. That’s because the running time of an algorithm on an input of
constant size is constant. We save ourselves a lot of extra writing by adopting this
convention.
Divide-and-Conquer
The divide-and-conquer method is a powerful strategy for designing asymptotically
efûcient algorithms. We saw an example of divide-and-conquer in Section 2.3.1
when learning about merge sort. In this chapter, we’ll explore applications of the
divide-and-conquer method and acquire valuable mathematical tools that you can
use to solve the recurrences that arise when analyzing divide-and-conquer algorithms.
Recall that for divide-and-conquer, you solve a given problem (instance) recursively.
If the problem is small enough4the base case4you just solve it directly
without recursing. Otherwise4the recursive case4you perform three characteristic steps:
Divide the problem into one or more subproblems that are smaller instances of the
same problem.
Conquer the subproblems by solving them recursively.
Combine the subproblem solutions to form a solution to the original problem.
A divide-and-conquer algorithm breaks down a large problem into smaller subproblems,
which themselves may be broken down into even smaller subproblems,
and so forth. The recursion bottoms out when it reaches a base case and the sub-
problem is small enough to solve directly without further recursing.
Recurrences
To analyze recursive divide-and-conquer algorithms, we’ll need some mathematical tools. A recurrence is an equation that describes a function in terms of its
value on other, typically smaller, arguments. Recurrences go hand in hand with
the divide-and-conquer method because they give us a natural way to characterize
the running times of recursive algorithms mathematically. You saw an example
of a recurrence in Section 2.3.2 when we analyzed the worst-case running time of
merge sort
Algorithmic recurrences
We’ll be particularly interested in recurrences that describe the running times of
divide-and-conquer algorithms. A recurrence T .n/ is algorithmic if, for every
sufûciently large threshold constant n 0 > 0, the following two properties hold:
1. For all n < n 0 , we have T .n/ D ‚.1/.
2. For all n  n 0 , every path of recursion terminates in a deûned base case within
a ûnite number of recursive invocations.
Similar to how we sometimes abuse asymptotic notation (see page 60), when a
function is not deûned for all arguments, we understand that this deûnition is con-
strained to values of n for which T .n/ is deûned.
Why would a recurrence T .n/ that represents a (correct) divide-and-conquer al-
gorithm’s worst-case running time satisfy these properties for all sufûciently large
threshold constants? The ûrst property says that there exist constants c 1 ; c 2 such
that 0 < c 1 හ T .n/ හ c 2 for n < n 0 . For every legal input, the algorithm must
output the solution to the problem it’s solving in ûnite time (see Section 1.1). Thus
we can let c 1 be the minimum amount of time to call and return from a procedure,
which must be positive, because machine instructions need to be executed to in-
voke a procedure. The running time of the algorithm may not be deûned for some
values of n if there are no legal inputs of that size, but it must be deûned for at
least one, or else the <algorithm= doesn’t solve any problem. Thus we can let c 2 be
the algorithm’s maximum running time on any input of size n < n 0 , where n 0 is
78
Chapter 4 Divide-and-Conquer
sufûciently large that the algorithm solves at least one problem of size less than n 0 .
The maximum is well deûned, since there are at most a ûnite number of inputs of
size less than n 0 , and there is at least one if n 0 is sufûciently large. Consequently,
T .n/ satisûes the ûrst property. If the second property fails to hold for T .n/, then
the algorithm isn’t correct, because it would end up in an inûnite recursive loop or
otherwise fail to compute a solution. Thus, it stands to reason that a recurrence for
the worst-case running time of a correct divide-and-conquer algorithm would be
algorithmic.
Conventions for recurrences
We adopt the following convention:
Whenever a recurrence is stated without an explicit base case, we assume
that the recurrence is algorithmic.
That means you’re free to pick any sufûciently large threshold constant n 0 for the
range of base cases where T .n/ D ‚.1/. Interestingly, the asymptotic solutions of
most algorithmic recurrences you’re likely to see when analyzing algorithms don’t
depend on the choice of threshold constant, as long as it’s large enough to make
the recurrence well deûned.
Asymptotic solutions of algorithmic divide-and-conquer recurrences also don’t
tend to change when we drop any üoors or ceilings in a recurrence deûned on the
integers to convert it to a recurrence deûned on the reals. Section 4.7 gives a suf-
ûcient condition for ignoring üoors and ceilings that applies to most of the divide-
and-conquer recurrences you’re likely to see. Consequently, we’ll frequently state
algorithmic recurrences without üoors and ceilings. Doing so generally simpliûes
the statement of the recurrences, as well as any math that we do with them.
You may sometimes see recurrences that are not equations, but rather inequal-
ities, such as T .n/ හ 2T .n=2/ C ‚.n/. Because such a recurrence states only
an upper bound on T .n/, we express its solution using O-notation rather than
‚-notation. Similarly, if the inequality is reversed to T .n/  2T .n=2/ C ‚.n/,
then, because the recurrence gives only a lower bound on T .n/, we use �-notation
in its solution.
Divide-and-conquer and recurrences
This chapter illustrates the divide-and-conquer method by presenting and using
recurrences to analyze two divide-and-conquer algorithms for multiplying n  n
matrices. Section 4.1 presents a simple divide-and-conquer algorithm that solves
a matrix-multiplication problem of size n by breaking it into four subproblems of
size n=2, which it then solves recursively. The running time of the algorithm can
be characterized by the recurrence
Chapter 4 Divide-and-Conquer
79
T .n/ D 8T .n=2/ C ‚.1/ ;
which turns out to have the solution T .n/ D ‚.n 3 /. Although this divide-and-
conquer algorithm is no faster than the straightforward method that uses a triply
nested loop, it leads to an asymptotically faster divide-and-conquer algorithm due
to V. Strassen, which we’ll explore in Section 4.2. Strassen’s remarkable algorithm
divides a problem of size n into seven subproblems of size n=2 which it solves
recursively. The running time of Strassen’s algorithm can be described by the
recurrence
T .n/ D 7T .n=2/ C ‚.n 2 / ;
which has the solution T .n/ D ‚.n lg 7 / D O.n 2:81 /. Strassen’s algorithm beats
the straightforward looping method asymptotically.
These two divide-and-conquer algorithms both break a problem of size n into
several subproblems of size n=2. Although it is common when using divide-and-
conquer for all the subproblems to have the same size, that isn’t always the case.
Sometimes it’s productive to divide a problem of size n into subproblems of differ-
ent sizes, and then the recurrence describing the running time reüects the irregular-
ity. For example, consider a divide-and-conquer algorithm that divides a problem
of size n into one subproblem of size n=3 and another of size 2n=3, taking ‚.n/
time to divide the problem and combine the solutions to the subproblems. Then the
algorithm’s running time can be described by the recurrence
T .n/ D T .n=3/ C T .2n=3/ C ‚.n/ ;
which turns out to have solution T .n/ D ‚.n lg n/. We’ll even see an algorithm in
Chapter 9 that solves a problem of size n by recursively solving a subproblem of
size n=5 and another of size 7n=10, taking ‚.n/ time for the divide and combine
steps. Its performance satisûes the recurrence
T .n/ D T .n=5/ C T .7n=10/ C ‚.n/ ;
which has solution T .n/ D ‚.n/.
Although divide-and-conquer algorithms usually create subproblems with sizes
a constant fraction of the original problem size, that’s not always the case. For
example, a recursive version of linear search (see Exercise 2.1-4) creates just one
subproblem, with one element less than the original problem. Each recursive call
takes constant time plus the time to recursively solve a subproblem with one less
element, leading to the recurrence
T .n/ D T .n  1/ C ‚.1/ ;
which has solution T .n/ D ‚.n/. Nevertheless, the vast majority of efûcient
divide-and-conquer algorithms solve subproblems that are a constant fraction of
the size of the original problem, which is where we’ll focus our efforts.
80
Chapter 4 Divide-and-Conquer
Solving recurrences
After learning about divide-and-conquer algorithms for matrix multiplication in
Sections 4.1 and 4.2, we’ll explore several mathematical tools for solving recur-
rences4that is, for obtaining asymptotic ‚-, O-, or �-bounds on their solutions.
We want simple-to-use tools that can handle the most commonly occurring situa-
tions. But we also want general tools that work, perhaps with a little more effort,
for less common cases. This chapter offers four methods for solving recurrences:

In the substitution method (Section 4.3), you guess the form of a bound and
then use mathematical induction to prove your guess correct and solve for con-
stants. This method is perhaps the most robust method for solving recurrences,
but it also requires you to make a good guess and to produce an inductive proof.

The recursion-tree method (Section 4.4) models the recurrence as a tree whose
nodes represent the costs incurred at various levels of the recursion. To solve
the recurrence, you determine the costs at each level and add them up, perhaps
using techniques for bounding summations from Section A.2. Even if you don’t
use this method to formally prove a bound, it can be helpful in guessing the form
of the bound for use in the substitution method.

The master method (Sections 4.5 and 4.6) is the easiest method, when it applies.
It provides bounds for recurrences of the form
T .n/ D aT .n=b/ C f .n/ ;
where a > 0 and b > 1 are constants and f .n/ is a given <driving= function.
This type of recurrence tends to arise more frequently in the study of algorithms
than any other. It characterizes a divide-and-conquer algorithm that creates
a subproblems, each of which is 1=b times the size of the original problem,
using f .n/ time for the divide and combine steps. To apply the master method,
you need to memorize three cases, but once you do, you can easily determine
asymptotic bounds on running times for many divide-and-conquer algorithms.

The Akra-Bazzi method (Section 4.7) is a general method for solving divide-
and-conquer recurrences. Although it involves calculus, it can be used to attack
more complicated recurrences than those addressed by the master method